name: video-downloader

version: '3.8'

# Synology NAS Optimized Configuration
# For use with Synology Container Manager (formerly Docker)

services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    container_name: video_db
    restart: unless-stopped
    env_file:
      - .env
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres_password}
      POSTGRES_DB: video_db
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - /volume1/docker/video-downloader/db_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    #ports:
    #  - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - video_network
    user: "1026:100"  # Synology user:group (adjust as needed)

  # Redis Queue
  redis:
    image: redis:7-alpine
    container_name: video_redis
    restart: unless-stopped
    command: redis-server --appendonly yes --save 60 1
    volumes:
      - /volume1/docker/video-downloader/redis_data:/data
    #ports:
    #  - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - video_network

  # API Gateway (FastAPI)
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: video_api
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - API_KEY=${API_KEY:-change-this-key}
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD:-postgres_password}@db:5432/video_db
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-10}
      - STORAGE_PATH=/downloads
    volumes:
      - /volume1/nsfw_video/video-downloader/downloads:/downloads
      - /volume1/docker/video-downloader/logs:/logs
    ports:
      - "52052:8000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - video_network
    user: "1026:100"  # Match your Synology user

  # Download Worker 1
  worker:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: video_worker_1
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD:-postgres_password}@db:5432/video_db
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_CONCURRENT_DOWNLOADS=${MAX_CONCURRENT_DOWNLOADS:-3}
      - MAX_DOWNLOAD_WORKERS=${MAX_DOWNLOAD_WORKERS:-10}
      - MAX_RETRY_ATTEMPTS=${MAX_RETRY_ATTEMPTS:-3}
      - FFMPEG_THREADS=${FFMPEG_THREADS:-4}
      - STORAGE_PATH=/downloads
    volumes:
      - /volume1/nsfw_video/video-downloader/downloads:/downloads
      - /volume1/docker/video-downloader/logs:/logs
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - video_network
    user: "1026:100"  # Match your Synology user

  # Download Worker 2
  worker2:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: video_worker_2
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD:-postgres_password}@db:5432/video_db
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_CONCURRENT_DOWNLOADS=${MAX_CONCURRENT_DOWNLOADS:-3}
      - MAX_DOWNLOAD_WORKERS=${MAX_DOWNLOAD_WORKERS:-10}
      - MAX_RETRY_ATTEMPTS=${MAX_RETRY_ATTEMPTS:-3}
      - FFMPEG_THREADS=${FFMPEG_THREADS:-4}
      - STORAGE_PATH=/downloads
    volumes:
      - /volume1/nsfw_video/video-downloader/downloads:/downloads
      - /volume1/docker/video-downloader/logs:/logs
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - video_network
    user: "1026:100"  # Match your Synology user

  # Periodic DB cleanup: keep latest 100 finished jobs
  db_cleanup:
    image: postgres:15-alpine
    container_name: video_db_cleanup
    restart: unless-stopped
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
    command:
      - sh
      - -ec
      - |
          while true; do
            echo "[db_cleanup] pruning finished jobs (keep latest 100)..."
            export PGPASSWORD="$${DB_PASSWORD:-postgres_password}"
            psql -h db -U postgres -d video_db -v ON_ERROR_STOP=1 -c "WITH keep AS (SELECT id FROM jobs WHERE status IN ('completed','failed') ORDER BY created_at DESC LIMIT 100) DELETE FROM jobs WHERE status IN ('completed','failed') AND id NOT IN (SELECT id FROM keep);"
            sleep "$${CLEANUP_INTERVAL_SECONDS:-3600}"
          done
    networks:
      - video_network

networks:
  video_network:
    driver: bridge

# Note: Synology manages volumes through host paths
# No named volumes used to avoid permission issues

